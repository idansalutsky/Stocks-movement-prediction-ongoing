{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Notebook Summary\n",
        "In this notebook I used a similar to the notebook in which I selected features, an LSTM algorithm that only on a sequence of the last X days (depending on the type of label) also, similar to the previous notebooks, I resampled the data to create a balance between the labeling and scaling of the data itself. You can see after each run the results of the models in different indices"
      ],
      "metadata": {
        "id": "R_vNiQcGI11d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###results\n",
        "The model results are better in all parameters than the results of the base algorithm, this time the model succeeds in the opposite way, better prediction for a larger time range. My hypothesis is that the weekly and monthly range are less noisy compared to the smaller range but still not \"too far\""
      ],
      "metadata": {
        "id": "nE0aW_sIa7eB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqPZcLuXTkBV",
        "outputId": "14ef3060-ceee-4f8b-e413-01f961201979"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vVkQGQsDqMUj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "Consecutive_stocks =  pd.read_csv('/content/drive/MyDrive/stocks_data/Consecutive_stocks.csv')\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import resample\n",
        "from collections import Counter\n",
        "\n",
        "def preprocess_data_train_val(df, label_col='label_week', seq_length=10, start_date='2022-12-31', exclude_columns=[], alpha=0.8,device = 'cpu'):\n",
        "\n",
        "    df = df.copy()\n",
        "    df = df[df[label_col].isna()==False].fillna(-1)\n",
        "\n",
        "    all_columns = df.columns\n",
        "    columns_to_keep = list(set(all_columns) - set(exclude_columns))\n",
        "\n",
        "    df = df[pd.to_datetime(df['date']) >= pd.to_datetime(start_date)]\n",
        "    min_date = pd.to_datetime(df['date']).min()\n",
        "    max_date = pd.to_datetime(df['date']).max()\n",
        "    num_days = (max_date - min_date).days\n",
        "    split_date = min_date + pd.DateOffset(days=int(alpha * num_days))\n",
        "    print(f'Train dates are {min_date} - {split_date}, test dates are {split_date} - {max_date}')\n",
        "\n",
        "    df.set_index(['date', 'Symbol'], inplace=True)\n",
        "    label_col_copy = df[label_col].copy()\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    df[columns_to_keep] = scaler.fit_transform(df[columns_to_keep])\n",
        "    df[label_col] = label_col_copy\n",
        "\n",
        "    train_data = df[pd.to_datetime(df.index.get_level_values('date')) <= split_date].loc[:, columns_to_keep]\n",
        "    test_data = df[pd.to_datetime(df.index.get_level_values('date')) > split_date].loc[:, columns_to_keep]\n",
        "\n",
        "    sequences_train, labels_train = [], []\n",
        "    for symbol in train_data.index.get_level_values('Symbol').unique():\n",
        "        symbol_data = train_data.loc[train_data.index.get_level_values('Symbol') == symbol]\n",
        "        symbol_sequences, symbol_labels = create_sequences(symbol_data[train_data.columns[:-1]], symbol_data[label_col], seq_length)\n",
        "        sequences_train.append(symbol_sequences)\n",
        "        labels_train.append(symbol_labels)\n",
        "\n",
        "    sequences_test, labels_test = [], []\n",
        "    for symbol in test_data.index.get_level_values('Symbol').unique():\n",
        "        symbol_data = test_data.loc[test_data.index.get_level_values('Symbol') == symbol]\n",
        "        symbol_sequences, symbol_labels = create_sequences(symbol_data[test_data.columns[:-1]], symbol_data[label_col], seq_length)\n",
        "        sequences_test.append(symbol_sequences)\n",
        "        labels_test.append(symbol_labels)\n",
        "\n",
        "    sequences_test = torch.cat(sequences_test).to(device)\n",
        "    labels_test = torch.cat(labels_test).to(device)\n",
        "    sequences_train = torch.cat(sequences_train).to(device)\n",
        "    labels_train = torch.cat(labels_train).to(device)\n",
        "    #Handlling unbalanced data with taking randonly equaly nuber of samples from each class\n",
        "\n",
        "    # Extract unique class labels\n",
        "    unique_classes = set(labels_train.cpu().numpy())\n",
        "\n",
        "    # Find the minimum count among classes\n",
        "    min_samples = min(labels_train.tolist().count(c) for c in unique_classes)\n",
        "\n",
        "    class_sequences = []\n",
        "    class_labels = []\n",
        "    for c in unique_classes:\n",
        "        indices = [i for i, label in enumerate(labels_train) if torch.equal(label, torch.tensor(c))]\n",
        "        indices_tensor = torch.tensor(indices)\n",
        "        random_indices_tensor = torch.randperm(indices_tensor.size(0))[:min_samples]\n",
        "        random_indices = indices_tensor[random_indices_tensor]\n",
        "\n",
        "        class_sequences.extend([sequences_train[i] for i in random_indices])\n",
        "        class_labels.extend([labels_train[i] for i in random_indices])\n",
        "\n",
        "    # Combine the sequences and labels for each class\n",
        "    sequences_train = torch.stack(class_sequences)\n",
        "    labels_train = torch.stack(class_labels)\n",
        "    unique_classes = set(labels_train.cpu().numpy())\n",
        "\n",
        "    return sequences_train, labels_train, sequences_test, labels_test\n",
        "\n",
        "\n",
        "def create_sequences(data, labels, seq_length=7):\n",
        "    sequences, next_labels = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        seq = data.iloc[i:i + seq_length]\n",
        "        label = labels.iloc[i + seq_length]\n",
        "        if label == -1:\n",
        "          continue\n",
        "        sequences.append(seq.values)\n",
        "        next_labels.append(label)\n",
        "\n",
        "    sequences = np.array(sequences, dtype=np.float32)\n",
        "    next_labels = np.array(next_labels, dtype=np.int64)\n",
        "\n",
        "    return torch.tensor(sequences).to(device), torch.tensor(next_labels).to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "py5Q544Hw6Ib"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report,confusion_matrix\n",
        "\n",
        "def lstm_pipline(df, label_col='label_week',columns_to_use=[], seq_length=10, hidden_size=64, num_layers=2, num_classes=8, lr=0.01, num_epochs=5, batch_size=128, alpha_lasso=0.01, alpha=0.8, start_date='2022-12-31',device = 'cpu'):\n",
        "    # Preprocess data and get train and test data\n",
        "    # Exclude some columns from the 'df' DataFrame\n",
        "    if label_col == 'label_week':\n",
        "        columns_to_exclude = ['label_day',  'label_month', 'Symbol', 'date','pct_change_week','pct_change_month', 'stock']\n",
        "    elif label_col == 'label_day':\n",
        "        columns_to_exclude = [ 'label_week', 'label_month', 'Symbol', 'date','pct_change_week','pct_change_month', 'stock']\n",
        "    elif label_col == 'label_month':\n",
        "        columns_to_exclude = ['label_day', 'label_week',  'Symbol', 'date','pct_change_week','pct_change_month', 'stock']\n",
        "\n",
        "\n",
        "    all_columns = df.columns\n",
        "    columns_to_exclude = list(set(all_columns) - set(columns_to_use+[label_col]))\n",
        "\n",
        "    print(f'Preprocessing and creating sequences, label column is {label_col}...')\n",
        "    sequences_train, labels_train, sequences_test, labels_test = preprocess_data_train_val(df, label_col=label_col, seq_length=seq_length, start_date=start_date, exclude_columns=columns_to_exclude, alpha=alpha)\n",
        "\n",
        "    # Create DataLoader for training\n",
        "    train_dataset = TensorDataset(sequences_train, labels_train)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Create DataLoader for testing\n",
        "    test_dataset = TensorDataset(sequences_test, labels_test)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    class LSTM(nn.Module):\n",
        "        def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "            super(LSTM, self).__init__()\n",
        "            self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "            self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "        def forward(self, x):\n",
        "            out, _ = self.lstm(x)\n",
        "            out = self.fc(out[:, -1, :])\n",
        "            return out\n",
        "\n",
        "    # Automatically determine the input size from the data\n",
        "    input_size = sequences_train.size(2)\n",
        "\n",
        "    model = LSTM(input_size, hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes)\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    best_loss = float('inf')  # Initialize with positive infinity\n",
        "    best_epoch = 0\n",
        "    best_model_state_dict = None\n",
        "\n",
        "    print('Start training')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, (inputs, labels) in enumerate(train_loader, 1):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Total loss with L1 regularization\n",
        "            loss = F.cross_entropy(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if i % 1500 == 0:\n",
        "                print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{i}/{len(train_loader)}], Loss: {running_loss / i:.4f}')\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
        "\n",
        "        # Check if the current epoch's loss is the best so far\n",
        "        if epoch_loss < best_loss:\n",
        "            best_loss = epoch_loss\n",
        "            best_epoch = epoch\n",
        "            best_model_state_dict = model.state_dict()\n",
        "\n",
        "    print(f'Best Loss: {best_loss:.4f} at Epoch [{best_epoch+1}/{num_epochs}]')\n",
        "\n",
        "    # Load the best model state\n",
        "    model.load_state_dict(best_model_state_dict)\n",
        "    print('Evaluating')\n",
        "\n",
        "    # In case i want to use this part to get predictions and there probabilities for the stocks\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    all_predictions, all_probabilities, all_real_labels = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            probabilities = F.softmax(outputs, dim=1)\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            probabilities = F.softmax(outputs, dim=1)\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_probabilities.extend(probabilities.cpu().numpy())\n",
        "            all_real_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_real_labels, all_predictions)\n",
        "    f1 = f1_score(all_real_labels, all_predictions, average='weighted')\n",
        "    report = classification_report(all_real_labels, all_predictions)\n",
        "    unique_classes = list(set(all_real_labels))\n",
        "    cm = confusion_matrix(all_real_labels, all_predictions, labels=unique_classes)\n",
        "\n",
        "    print(f'Accuracy {accuracy* 100:.2f}')\n",
        "    print(f'F1-{f1}')\n",
        "    print(f'report-')\n",
        "    print(report)\n",
        "    print('confusion_matrix-')\n",
        "    print(cm)\n",
        "\n",
        "    return model,accuracy, f1, report,cm\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yy4SCZxrYLgt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_test_ratio=0.8\n",
        "top_X_features = 20\n",
        "Nepochs = 5\n",
        "start_date='2021-12-31'\n",
        "hidden_size = 64\n",
        "batch_size = 64\n",
        "learning_rate = 0.01"
      ],
      "metadata": {
        "id": "9AnoT5VewlvY"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_df = pd.read_csv('/content/drive/MyDrive/stocks_data/selected_columns.csv')\n",
        "\n",
        "# Retrieve lists from loaded DataFrame\n",
        "columns_to_use_day = loaded_df['columns_to_use_day'].tolist()\n",
        "columns_to_use_week = loaded_df['columns_to_use_week'].tolist()\n",
        "columns_to_use_month = loaded_df['columns_to_use_month'].tolist()"
      ],
      "metadata": {
        "id": "uQOVrUo1LA_Z"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Day"
      ],
      "metadata": {
        "id": "2F1tzXXc7V_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_day,accuracy_day, f1_day, report_day,cm_day  = lstm_pipline(df=Consecutive_stocks, label_col='label_day',columns_to_use=columns_to_use_day ,seq_length=7, hidden_size=hidden_size, num_layers=2, num_classes=8, lr=learning_rate, num_epochs=Nepochs, batch_size=batch_size, alpha_lasso=0.01, alpha=train_test_ratio, start_date=start_date)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSA6qeXn1moI",
        "outputId": "69030484-fc13-4efd-f5b4-735964047c18"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing and creating sequences, label column is label_day...\n",
            "Start training\n",
            "Epoch [1/5], Batch [1500/1679], Loss: 1.9234\n",
            "Epoch [1/5], Loss: 1.9204\n",
            "Epoch [2/5], Batch [1500/1679], Loss: 1.8774\n",
            "Epoch [2/5], Loss: 1.8747\n",
            "Epoch [3/5], Batch [1500/1679], Loss: 1.8145\n",
            "Epoch [3/5], Loss: 1.8107\n",
            "Epoch [4/5], Batch [1500/1679], Loss: 1.7513\n",
            "Epoch [4/5], Loss: 1.7497\n",
            "Epoch [5/5], Batch [1500/1679], Loss: 1.7100\n",
            "Epoch [5/5], Loss: 1.7081\n",
            "Best Loss: 1.7081 at Epoch [5/5]\n",
            "Evaluating\n",
            "Accuracy 28.51\n",
            "F1-0.2978056493854241\n",
            "report-\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.04      0.17      0.07      2784\n",
            "           1       0.07      0.10      0.09     13728\n",
            "           2       0.11      0.06      0.08     36673\n",
            "           3       0.42      0.19      0.26    146096\n",
            "           4       0.46      0.47      0.46    156735\n",
            "           5       0.11      0.25      0.15     33878\n",
            "           6       0.07      0.08      0.08     14853\n",
            "           7       0.05      0.39      0.08      4540\n",
            "\n",
            "    accuracy                           0.29    409287\n",
            "   macro avg       0.17      0.21      0.16    409287\n",
            "weighted avg       0.35      0.29      0.30    409287\n",
            "\n",
            "confusion_matrix-\n",
            "[[  471   248    81    82   159   326   248  1169]\n",
            " [ 1256  1409   601   726  1164  2898  1503  4171]\n",
            " [ 1545  2735  2161  4167  8225  9427  2747  5666]\n",
            " [ 2130  4935  6595 27895 65230 26654  4631  8026]\n",
            " [ 2367  5172  6852 28260 73156 27426  4895  8607]\n",
            " [ 1342  2324  2164  4410  8441  8581  2145  4471]\n",
            " [ 1167  1549   923  1319  1769  3201  1256  3669]\n",
            " [  709   509   202   136   227   630   365  1762]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Week"
      ],
      "metadata": {
        "id": "F78D4JNV7YkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_week,accuracy_week, f1_week, report_week, cm_week = lstm_pipline(df=Consecutive_stocks, label_col='label_week',columns_to_use=columns_to_use_week, seq_length=12, hidden_size=hidden_size*2, num_layers=2, num_classes=8, lr=learning_rate, num_epochs=Nepochs, batch_size=batch_size*2, alpha_lasso=0.01, alpha=train_test_ratio, start_date=start_date)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jj_5KVeg2XDS",
        "outputId": "52ddd0f3-19f9-423f-d593-88da1fff2179"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing and creating sequences, label column is label_week...\n",
            "Start training\n",
            "Epoch [1/5], Batch [1500/6863], Loss: 1.3847\n",
            "Epoch [1/5], Batch [3000/6863], Loss: 1.3555\n",
            "Epoch [1/5], Batch [4500/6863], Loss: 1.3412\n",
            "Epoch [1/5], Batch [6000/6863], Loss: 1.3309\n",
            "Epoch [1/5], Loss: 1.3253\n",
            "Epoch [2/5], Batch [1500/6863], Loss: 1.2802\n",
            "Epoch [2/5], Batch [3000/6863], Loss: 1.2771\n",
            "Epoch [2/5], Batch [4500/6863], Loss: 1.2744\n",
            "Epoch [2/5], Batch [6000/6863], Loss: 1.2720\n",
            "Epoch [2/5], Loss: 1.2705\n",
            "Epoch [3/5], Batch [1500/6863], Loss: 1.2523\n",
            "Epoch [3/5], Batch [3000/6863], Loss: 1.2492\n",
            "Epoch [3/5], Batch [4500/6863], Loss: 1.2466\n",
            "Epoch [3/5], Batch [6000/6863], Loss: 1.2441\n",
            "Epoch [3/5], Loss: 1.2429\n",
            "Epoch [4/5], Batch [1500/6863], Loss: 1.2303\n",
            "Epoch [4/5], Batch [3000/6863], Loss: 1.2285\n",
            "Epoch [4/5], Batch [4500/6863], Loss: 1.2265\n",
            "Epoch [4/5], Batch [6000/6863], Loss: 1.2257\n",
            "Epoch [4/5], Loss: 1.2255\n",
            "Epoch [5/5], Batch [1500/6863], Loss: 1.2144\n",
            "Epoch [5/5], Batch [3000/6863], Loss: 1.2157\n",
            "Epoch [5/5], Batch [4500/6863], Loss: 1.2158\n",
            "Epoch [5/5], Batch [6000/6863], Loss: 1.2158\n",
            "Epoch [5/5], Loss: 1.2156\n",
            "Best Loss: 1.2156 at Epoch [5/5]\n",
            "Evaluating\n",
            "Accuracy 41.12\n",
            "F1-0.40558438819784093\n",
            "report-\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.73      0.60     19167\n",
            "           1       0.39      0.48      0.43     39414\n",
            "           2       0.29      0.37      0.33     47432\n",
            "           3       0.45      0.30      0.36     73377\n",
            "           4       0.49      0.35      0.41     80363\n",
            "           5       0.33      0.41      0.36     46628\n",
            "           6       0.44      0.35      0.39     39867\n",
            "           7       0.49      0.76      0.59     24252\n",
            "\n",
            "    accuracy                           0.41    370500\n",
            "   macro avg       0.42      0.47      0.43    370500\n",
            "weighted avg       0.42      0.41      0.41    370500\n",
            "\n",
            "confusion_matrix-\n",
            "[[14028  3566   743    75    87   127   102   439]\n",
            " [ 8082 19113  8277  1364   655   759   359   805]\n",
            " [ 2346 13467 17701  7406  2902  1935   712   963]\n",
            " [ 1234  7074 18809 21894 14636  6652  1633  1445]\n",
            " [  856  3298  9494 14404 28232 17537  3948  2594]\n",
            " [  420  1167  3292  2589  9245 19047  7459  3409]\n",
            " [  394   784  1739   641  2067 10659 13955  9628]\n",
            " [  341   288   497    63   183  1156  3341 18383]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Month"
      ],
      "metadata": {
        "id": "yOSMRS0T7aqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_month,accuracy_month, f1_month, report_month, cm_month = lstm_pipline(df=Consecutive_stocks, label_col='label_month',columns_to_use=columns_to_use_month, seq_length=20, hidden_size=hidden_size*4, num_layers=2, num_classes=8, lr=learning_rate, num_epochs=Nepochs, batch_size=batch_size*4, alpha_lasso=0.01, alpha=train_test_ratio, start_date=start_date)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wgl7ZBxO2oOy",
        "outputId": "6e908b76-3169-4fcb-94f9-fab6e7d6e05b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing and creating sequences, label column is label_month...\n",
            "Start training\n",
            "Epoch [1/5], Batch [1500/3589], Loss: 1.2331\n",
            "Epoch [1/5], Batch [3000/3589], Loss: 1.2141\n",
            "Epoch [1/5], Loss: 1.2086\n",
            "Epoch [2/5], Batch [1500/3589], Loss: 1.1682\n",
            "Epoch [2/5], Batch [3000/3589], Loss: 1.1622\n",
            "Epoch [2/5], Loss: 1.1593\n",
            "Epoch [3/5], Batch [1500/3589], Loss: 1.1347\n",
            "Epoch [3/5], Batch [3000/3589], Loss: 1.1293\n",
            "Epoch [3/5], Loss: 1.1274\n",
            "Epoch [4/5], Batch [1500/3589], Loss: 1.1129\n",
            "Epoch [4/5], Batch [3000/3589], Loss: 1.1099\n",
            "Epoch [4/5], Loss: 1.1100\n",
            "Epoch [5/5], Batch [1500/3589], Loss: 1.0989\n",
            "Epoch [5/5], Batch [3000/3589], Loss: 1.1044\n",
            "Epoch [5/5], Loss: 1.1047\n",
            "Best Loss: 1.1047 at Epoch [5/5]\n",
            "Evaluating\n",
            "Accuracy 65.03\n",
            "F1-0.6523654576011914\n",
            "report-\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86     37726\n",
            "           1       0.53      0.60      0.56     22524\n",
            "           2       0.35      0.35      0.35     14269\n",
            "           3       0.37      0.38      0.37     16988\n",
            "           4       0.47      0.37      0.41     21586\n",
            "           5       0.35      0.43      0.39     19401\n",
            "           6       0.54      0.54      0.54     35300\n",
            "           7       0.90      0.86      0.87     78029\n",
            "\n",
            "    accuracy                           0.65    245823\n",
            "   macro avg       0.55      0.55      0.55    245823\n",
            "weighted avg       0.66      0.65      0.65    245823\n",
            "\n",
            "confusion_matrix-\n",
            "[[32766  4101   354   190    70    81    84    80]\n",
            " [ 4330 13491  2847  1059   312   244   159    82]\n",
            " [  550  4405  4988  2837   714   468   233    74]\n",
            " [  293  1805  3647  6373  2852  1395   469   154]\n",
            " [  150   795  1394  4507  7931  4975  1495   339]\n",
            " [   91   337   433  1467  3423  8414  4543   693]\n",
            " [   97   276   288   749  1322  7006 19185  6377]\n",
            " [  139   185   115   249   278  1248  9093 66722]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_probability_for_individual_classes(original_conf_matrix, interval = 'week'):\n",
        "\n",
        "    # Initialize an array to store the probability for each class\n",
        "    probability_per_class_dict = {}\n",
        "\n",
        "    for i in range(8):\n",
        "        # Extract the confusion matrix for the current class\n",
        "        class_conf_matrix = original_conf_matrix[:, i]\n",
        "\n",
        "        # Calculate the total counts for the current class\n",
        "        total_class = class_conf_matrix.sum()\n",
        "        if i in [0,1,2,3]:\n",
        "          class_count = class_conf_matrix[:4].sum()\n",
        "\n",
        "        else:\n",
        "          class_count = class_conf_matrix[4:].sum()\n",
        "\n",
        "        # Calculate the probability of the current class being positive\n",
        "        probability_class = class_count / total_class\n",
        "\n",
        "        # Append the probability array for the current class to the result\n",
        "        probability_per_class_dict[interval +\" \"+ str(i)]= probability_class\n",
        "\n",
        "    print(f\"\\n{interval}ly probability for Each Class to be in its category- up/down\")\n",
        "    print(probability_per_class_dict)\n",
        "\n",
        "    return probability_per_class_dict\n"
      ],
      "metadata": {
        "id": "5TlPouAQiH4S"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_probabilities_day = calculate_probability_for_individual_classes(cm_day, 'Day')\n",
        "\n",
        "class_probabilities_week = calculate_probability_for_individual_classes(cm_week,'Week')\n",
        "\n",
        "class_probabilities_month = calculate_probability_for_individual_classes(cm_month,'Month')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Hi_iPTiKJ00",
        "outputId": "eb85bf1f-cd4c-43c7-973a-d36d98949d8e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dayly probability for Each Class to be in its category- up/down\n",
            "{'Day 0': 0.4916719759716028, 'Day 1': 0.4939886658545628, 'Day 2': 0.48204709127126, 'Day 3': 0.4906336293753265, 'Day 4': 0.5278302214420569, 'Day 5': 0.5033673224416563, 'Day 6': 0.48684654300168634, 'Day 7': 0.4930342825177806}\n",
            "\n",
            "Weekly probability for Each Class to be in its category- up/down\n",
            "{'Week 0': 0.9274033428396087, 'Week 1': 0.8864368193285067, 'Week 2': 0.7519157088122606, 'Week 3': 0.6346312660004955, 'Week 4': 0.684865619666592, 'Week 5': 0.8363111694774675, 'Week 6': 0.9109460788980926, 'Week 7': 0.9030425317262253}\n",
            "\n",
            "Monthly probability for Each Class to be in its category- up/down\n",
            "{'Month 0': 0.9875832986255727, 'Month 1': 0.9372711163614885, 'Month 2': 0.841461680648372, 'Month 3': 0.6000229476220527, 'Month 4': 0.7664181753638623, 'Month 5': 0.9081868154924259, 'Month 6': 0.973199852528289, 'Month 7': 0.9947665758645214}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}